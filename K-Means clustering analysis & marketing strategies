import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# load UCI Online Retail dataset
# æ•°æ®å¯ä» https://archive.ics.uci.edu/ml/datasets/Online+Retail ä¸‹è½½
df = pd.read_excel('Online Retail.xlsx')

print("===data info ===")
print(f"æ•°æ®å½¢çŠ¶ï¼š{df.shape}")
print(f"\næ•°æ®ç±»å‹ï¼š\n{df.dtypes}")
print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡ï¼š\n{df.isnull().sum()}")
print(f"\næ•°æ®é¢„è§ˆï¼š\n{df.head()}")

# æ•°æ®è´¨é‡é—®é¢˜è¯†åˆ«
print("\n=== data quality ===")
print(f"é‡å¤è®°å½•æ•°ï¼š{df.duplicated().sum()}")
print(f"CustomerIDç¼ºå¤±ï¼š{df['CustomerID'].isnull().sum()} ({df['CustomerID'].isnull().sum()/len(df)*100:.1f}%)")
print(f"è´Ÿæ•°æ•°é‡è®°å½•ï¼š{(df['Quantity'] < 0).sum()}")
print(f"é›¶ä»·æ ¼è®°å½•ï¼š{(df['UnitPrice'] == 0).sum()}")

# åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
print(f"\n=== åŸºç¡€ç»Ÿè®¡ ===")
print(f"å”¯ä¸€å®¢æˆ·æ•°ï¼š{df['CustomerID'].nunique()}")
print(f"å”¯ä¸€äº§å“æ•°ï¼š{df['StockCode'].nunique()}")
print(f"æ—¶é—´è·¨åº¦ï¼š{df['InvoiceDate'].min()} è‡³ {df['InvoiceDate'].max()}")
print(f"å›½å®¶æ•°é‡ï¼š{df['Country'].nunique()}")
                

# æ•°æ®æ¸…æ´—æ­¥éª¤
def clean_retail_data(df):
    """
    æ¸…æ´—UCI Online Retailæ•°æ®é›†
    """
    print("åŸå§‹æ•°æ®å½¢çŠ¶:", df.shape)
    
    # 1. åˆ é™¤CustomerIDç¼ºå¤±çš„è®°å½•
    df_clean = df.dropna(subset=['CustomerID'])
    print(f"åˆ é™¤CustomerIDç¼ºå¤±å: {df_clean.shape}")
    
    # 2. åˆ é™¤é€€è´§è®°å½•ï¼ˆæ•°é‡ä¸ºè´Ÿæ•°ï¼‰
    df_clean = df_clean[df_clean['Quantity'] > 0]
    print(f"åˆ é™¤é€€è´§è®°å½•å: {df_clean.shape}")
    
    # 3. åˆ é™¤ä»·æ ¼ä¸º0æˆ–è´Ÿæ•°çš„è®°å½•
    df_clean = df_clean[df_clean['UnitPrice'] > 0]
    print(f"åˆ é™¤å¼‚å¸¸ä»·æ ¼å: {df_clean.shape}")
    
    # 4. åˆ é™¤æ˜æ˜¾çš„æµ‹è¯•æ•°æ®ï¼ˆå¦‚POSTã€DOTç­‰ï¼‰
    test_codes = ['POST', 'DOT', 'M', 'S', 'AMAZONFEE', 'DCGSSBOY', 'DCGSSGIRL']
    df_clean = df_clean[~df_clean['StockCode'].isin(test_codes)]
    print(f"åˆ é™¤æµ‹è¯•æ•°æ®å: {df_clean.shape}")
    
    # 5. åˆ é™¤Descriptionç¼ºå¤±çš„è®°å½•
    df_clean = df_clean.dropna(subset=['Description'])
    print(f"åˆ é™¤Descriptionç¼ºå¤±å: {df_clean.shape}")
    
    # 6. å¤„ç†é‡å¤è®°å½•
    df_clean = df_clean.drop_duplicates()
    print(f"åˆ é™¤é‡å¤è®°å½•å: {df_clean.shape}")
    
    # 7. è®¡ç®—æ€»é‡‘é¢
    df_clean['TotalAmount'] = df_clean['Quantity'] * df_clean['UnitPrice']
    
    # 8. è½¬æ¢æ•°æ®ç±»å‹
    df_clean['CustomerID'] = df_clean['CustomerID'].astype(int)
    df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])
    
    return df_clean

# åº”ç”¨æ¸…æ´—å‡½æ•°
df_clean = clean_retail_data(df)

# æ¸…æ´—åçš„æ•°æ®æ¦‚è§ˆ
print("\n=== æ¸…æ´—åæ•°æ®æ¦‚è§ˆ ===")
print(f"æœ€ç»ˆæ•°æ®å½¢çŠ¶: {df_clean.shape}")
print(f"å”¯ä¸€å®¢æˆ·æ•°: {df_clean['CustomerID'].nunique()}")
print(f"æ•°æ®å®Œæ•´æ€§: {(1 - df_clean.isnull().sum().sum()/(df_clean.shape[0]*df_clean.shape[1]))*100:.1f}%")


# æ¢ç´¢æ€§æ•°æ®åˆ†æ
def exploratory_analysis(df):
    """
    å¯¹æ¸…æ´—åçš„æ•°æ®è¿›è¡Œæ¢ç´¢æ€§åˆ†æ
    """
    # 1. é”€å”®é‡‘é¢åˆ†å¸ƒ
    print("=== é”€å”®é‡‘é¢åˆ†å¸ƒ ===")
    print(f"æ€»é”€å”®é¢: Â£{df['TotalAmount'].sum():,.2f}")
    print(f"å¹³å‡è®¢å•é‡‘é¢: Â£{df['TotalAmount'].mean():.2f}")
    print(f"ä¸­ä½æ•°è®¢å•é‡‘é¢: Â£{df['TotalAmount'].median():.2f}")
    
    # 2. å®¢æˆ·æ¶ˆè´¹åˆ†å¸ƒ
    customer_summary = df.groupby('CustomerID').agg({
        'TotalAmount': 'sum',
        'InvoiceNo': 'nunique',
        'Quantity': 'sum',
        'InvoiceDate': ['min', 'max']
    }).round(2)
    
    customer_summary.columns = ['TotalSpent', 'OrderCount', 'ItemCount', 'FirstPurchase', 'LastPurchase']
    
    print("\n=== å®¢æˆ·æ¶ˆè´¹ç»Ÿè®¡ ===")
    print(customer_summary.describe())
    
    # 3. äº§å“åˆ†æ
    product_summary = df.groupby('StockCode').agg({
        'TotalAmount': 'sum',
        'Quantity': 'sum',
        'CustomerID': 'nunique'
    }).sort_values('TotalAmount', ascending=False)
    
    print("\n=== çƒ­é”€äº§å“TOP10 ===")
    print(product_summary.head(10))
    
    # 4. å›½å®¶åˆ†å¸ƒ
    country_summary = df.groupby('Country').agg({
        'TotalAmount': 'sum',
        'CustomerID': 'nunique'
    }).sort_values('TotalAmount', ascending=False)
    
    print("\n=== å›½å®¶é”€å”®åˆ†å¸ƒTOP10 ===")
    print(country_summary.head(10))
    
    return customer_summary

customer_data = exploratory_analysis(df_clean)
                

# RFMç‰¹å¾å·¥ç¨‹
def create_rfm_features(df):
    """
    åŸºäºäº¤æ˜“æ•°æ®åˆ›å»ºRFMç‰¹å¾
    """
    # è®¾ç½®åˆ†æåŸºå‡†æ—¥æœŸï¼ˆæ•°æ®é›†æœ€åæ—¥æœŸçš„æ¬¡æ—¥ï¼‰
    snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)
    print(f"åˆ†æåŸºå‡†æ—¥æœŸ: {snapshot_date}")
    
    # æŒ‰å®¢æˆ·èšåˆè®¡ç®—RFM
    rfm = df.groupby('CustomerID').agg({
        'InvoiceDate': lambda x: (snapshot_date - x.max()).days,  # Recency
        'InvoiceNo': 'nunique',  # Frequency  
        'TotalAmount': 'sum'  # Monetary
    }).round(2)
    
    # é‡å‘½ååˆ—
    rfm.columns = ['Recency', 'Frequency', 'Monetary']
    
    # æ·»åŠ è¡ç”Ÿç‰¹å¾
    rfm['AvgOrderValue'] = (df.groupby('CustomerID')['TotalAmount'].sum() / 
                           df.groupby('CustomerID')['InvoiceNo'].nunique()).round(2)
    
    # è®¡ç®—è´­ä¹°å¤©æ•°è·¨åº¦
    date_range = df.groupby('CustomerID')['InvoiceDate'].agg(['min', 'max'])
    rfm['DaySpan'] = (date_range['max'] - date_range['min']).dt.days
    
    # è®¡ç®—è´­ä¹°æ´»è·ƒåº¦ï¼ˆé¢‘æ¬¡/å¤©æ•°è·¨åº¦ï¼‰
    rfm['ActivityRate'] = (rfm['Frequency'] / (rfm['DaySpan'] + 1)).round(4)
    
    # æ·»åŠ å›½å®¶ä¿¡æ¯
    country_info = df.groupby('CustomerID')['Country'].first()
    rfm['Country'] = country_info
    
    return rfm

# åˆ›å»ºRFMæ•°æ®
rfm_data = create_rfm_features(df_clean)

print("=== RFMæ•°æ®æ¦‚è§ˆ ===")
print(f"RFMæ•°æ®å½¢çŠ¶: {rfm_data.shape}")
print(f"\nRFMæè¿°æ€§ç»Ÿè®¡:")
print(rfm_data[['Recency', 'Frequency', 'Monetary', 'AvgOrderValue']].describe())

# æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
print(f"\n=== RFMåˆ†å¸ƒæ£€æŸ¥ ===")
print(f"RecencyèŒƒå›´: {rfm_data['Recency'].min()} - {rfm_data['Recency'].max()} å¤©")
print(f"FrequencyèŒƒå›´: {rfm_data['Frequency'].min()} - {rfm_data['Frequency'].max()} æ¬¡")
print(f"MonetaryèŒƒå›´: Â£{rfm_data['Monetary'].min():.2f} - Â£{rfm_data['Monetary'].max():,.2f}")
                
# åˆ›å»ºæ›´å¤šè¥é”€ç›¸å…³ç‰¹å¾
def advanced_feature_engineering(rfm_data, df_clean):
    """
    åˆ›å»ºé«˜çº§è¥é”€åˆ†æç‰¹å¾
    """
    # 1. å®¢æˆ·ç”Ÿå‘½å‘¨æœŸç‰¹å¾
    rfm_data['CustomerLifetime'] = rfm_data['DaySpan'] + 1
    rfm_data['PurchaseInterval'] = rfm_data['CustomerLifetime'] / rfm_data['Frequency']
    
    # 2. å®¢æˆ·ä»·å€¼è¯„åˆ† (ç®€åŒ–çš„CLV)
    rfm_data['CLV_Score'] = (rfm_data['Monetary'] * rfm_data['Frequency'] / 
                            (rfm_data['Recency'] + 1)).round(2)
    
    # 3. è´­ä¹°è¡Œä¸ºç‰¹å¾
    item_stats = df_clean.groupby('CustomerID').agg({
        'Quantity': ['mean', 'std', 'max'],
        'UnitPrice': ['mean', 'std', 'max'],
        'StockCode': 'nunique'
    })
    
    item_stats.columns = ['AvgQuantity', 'StdQuantity', 'MaxQuantity',
                         'AvgPrice', 'StdPrice', 'MaxPrice', 'ProductVariety']
    
    # 4. æ—¶é—´è¡Œä¸ºç‰¹å¾
    time_stats = df_clean.groupby('CustomerID')['InvoiceDate'].agg([
        lambda x: x.dt.hour.mode().iloc[0] if len(x.dt.hour.mode()) > 0 else 12,  # åå¥½è´­ä¹°æ—¶é—´
        lambda x: x.dt.dayofweek.mode().iloc[0] if len(x.dt.dayofweek.mode()) > 0 else 1,  # åå¥½è´­ä¹°æ˜ŸæœŸ
        lambda x: x.dt.month.nunique()  # æ´»è·ƒæœˆä»½æ•°
    ])
    
    time_stats.columns = ['PreferredHour', 'PreferredDayOfWeek', 'ActiveMonths']
    
    # 5. åˆå¹¶æ‰€æœ‰ç‰¹å¾
    rfm_enhanced = rfm_data.join(item_stats).join(time_stats)
    
    # 6. å¡«å……ç¼ºå¤±å€¼
    rfm_enhanced = rfm_enhanced.fillna(rfm_enhanced.median())
    
    return rfm_enhanced

# åˆ›å»ºå¢å¼ºç‰¹å¾é›†
rfm_enhanced = advanced_feature_engineering(rfm_data, df_clean)

print("=== å¢å¼ºç‰¹å¾é›†æ¦‚è§ˆ ===")
print(f"ç‰¹å¾æ•°é‡: {rfm_enhanced.shape[1]}")
print(f"æ•°å€¼ç‰¹å¾: {rfm_enhanced.select_dtypes(include=[np.number]).columns.tolist()}")

# ç‰¹å¾ç›¸å…³æ€§åˆ†æ
numeric_features = rfm_enhanced.select_dtypes(include=[np.number])
correlation_matrix = numeric_features.corr()

print(f"\n=== å…³é”®ç‰¹å¾ç›¸å…³æ€§ ===")
print("ä¸Monetaryå¼ºç›¸å…³çš„ç‰¹å¾:")
monetary_corr = correlation_matrix['Monetary'].abs().sort_values(ascending=False)
print(monetary_corr.head(8))
                

# ç‰¹å¾é€‰æ‹©ç­–ç•¥
def select_clustering_features(rfm_enhanced):
    """
    é€‰æ‹©èšç±»çš„æ ¸å¿ƒç‰¹å¾
    """
    # æ ¸å¿ƒRFMç‰¹å¾
    core_features = ['Recency', 'Frequency', 'Monetary']
    
    # ä»·å€¼ç‰¹å¾
    value_features = ['AvgOrderValue', 'CLV_Score']
    
    # è¡Œä¸ºç‰¹å¾
    behavior_features = ['ProductVariety', 'ActivityRate', 'PurchaseInterval']
    
    # ç»„åˆæœ€ç»ˆç‰¹å¾é›†
    clustering_features = core_features + value_features + behavior_features
    
    print(f"é€‰æ‹©çš„èšç±»ç‰¹å¾: {clustering_features}")
    
    # æ£€æŸ¥ç‰¹å¾è´¨é‡
    feature_data = rfm_enhanced[clustering_features]
    
    print(f"\nç‰¹å¾è´¨é‡æ£€æŸ¥:")
    print(f"ç¼ºå¤±å€¼: {feature_data.isnull().sum().sum()}")
    print(f"æ— ç©·å€¼: {np.isinf(feature_data.select_dtypes(include=[np.number])).sum().sum()}")
    
    return clustering_features, feature_data

# é€‰æ‹©ç‰¹å¾
selected_features, feature_matrix = select_clustering_features(rfm_enhanced)

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(feature_matrix)

# åˆ›å»ºæ ‡å‡†åŒ–åçš„DataFrame
df_scaled = pd.DataFrame(X_scaled, columns=selected_features, index=feature_matrix.index)

print(f"\n=== æ ‡å‡†åŒ–åæ•°æ®æ¦‚è§ˆ ===")
print(f"æ•°æ®å½¢çŠ¶: {df_scaled.shape}")
print(f"ç‰¹å¾å‡å€¼ (åº”æ¥è¿‘0): {df_scaled.mean().round(3).tolist()}")
print(f"ç‰¹å¾æ ‡å‡†å·® (åº”æ¥è¿‘1): {df_scaled.std().round(3).tolist()}")
                
ğŸ“Š æœ€ä¼˜Kå€¼ç¡®å®š

# è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°åˆ†æ
def find_optimal_k(X, max_k=10):
    """
    ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°ç¡®å®šæœ€ä¼˜Kå€¼
    """
    wcss = []  # ç°‡å†…å¹³æ–¹å’Œ
    silhouette_scores = []  # è½®å»“ç³»æ•°
    k_range = range(2, max_k + 1)
    
    for k in k_range:
        # K-Meansèšç±»
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)
        cluster_labels = kmeans.fit_predict(X)
        
        # è®¡ç®—WCSS
        wcss.append(kmeans.inertia_)
        
        # è®¡ç®—è½®å»“ç³»æ•°
        sil_score = silhouette_score(X, cluster_labels)
        silhouette_scores.append(sil_score)
        
        print(f"K={k}: WCSS={kmeans.inertia_:.2f}, Silhouette={sil_score:.3f}")
    
    # æ‰¾åˆ°è‚˜éƒ¨ç‚¹ (WCSSäºŒé˜¶å¯¼æ•°æœ€å¤§ç‚¹)
    wcss_diff = np.diff(wcss)
    wcss_diff2 = np.diff(wcss_diff)
    elbow_k = np.argmax(wcss_diff2) + 3  # +3å› ä¸ºä»k=2å¼€å§‹ä¸”æœ‰ä¸¤æ¬¡diff
    
    # æœ€ä½³è½®å»“ç³»æ•°å¯¹åº”çš„K
    best_sil_k = k_range[np.argmax(silhouette_scores)]
    
    return {
        'k_range': list(k_range),
        'wcss': wcss,
        'silhouette_scores': silhouette_scores,
        'elbow_k': elbow_k,
        'best_silhouette_k': best_sil_k,
        'best_silhouette_score': max(silhouette_scores)
    }

# å¯»æ‰¾æœ€ä¼˜Kå€¼
k_analysis = find_optimal_k(X_scaled, max_k=12)

print(f"\n=== Kå€¼é€‰æ‹©ç»“æœ ===")
print(f"è‚˜éƒ¨æ³•åˆ™å»ºè®®Kå€¼: {k_analysis['elbow_k']}")
print(f"æœ€ä½³è½®å»“ç³»æ•°Kå€¼: {k_analysis['best_silhouette_k']}")
print(f"æœ€ä½³è½®å»“ç³»æ•°: {k_analysis['best_silhouette_score']:.3f}")

# æ ¹æ®ä¸šåŠ¡åœºæ™¯å’Œç®—æ³•ç»“æœé€‰æ‹©K=5
optimal_k = 5
print(f"\né€‰æ‹©K={optimal_k}çš„ç†ç”±:")
print("1. è½®å»“ç³»æ•°åœ¨K=4,5æ—¶è¾ƒä¼˜")
print("2. ä¸šåŠ¡ä¸Š5ä¸ªç¾¤ä½“ä¾¿äºè¥é”€ç­–ç•¥åˆ¶å®š")
print("3. ç¬¦åˆç»å…¸çš„å®¢æˆ·åˆ†å±‚æ¨¡å‹")
                
# è®­ç»ƒæœ€ç»ˆK-Meansæ¨¡å‹
final_k = 5
kmeans_final = KMeans(
    n_clusters=final_k, 
    random_state=42, 
    n_init=10, 
    max_iter=300,
    algorithm='lloyd'  # æ˜ç¡®æŒ‡å®šç®—æ³•
)

# æ‹Ÿåˆæ¨¡å‹å¹¶é¢„æµ‹
cluster_labels = kmeans_final.fit_predict(X_scaled)

# æ¨¡å‹è¯„ä¼°
final_silhouette = silhouette_score(X_scaled, cluster_labels)
final_inertia = kmeans_final.inertia_

# å°†èšç±»ç»“æœæ·»åŠ åˆ°åŸå§‹æ•°æ®
rfm_enhanced['Cluster'] = cluster_labels

print(f"=== æœ€ç»ˆæ¨¡å‹ç»“æœ ===")
print(f"è½®å»“ç³»æ•°: {final_silhouette:.3f}")
print(f"ç°‡å†…å¹³æ–¹å’Œ: {final_inertia:.2f}")
print(f"èšç±»åˆ†å¸ƒ:")
cluster_distribution = pd.Series(cluster_labels).value_counts().sort_index()
for cluster, count in cluster_distribution.items():
    percentage = count / len(cluster_labels) * 100
    print(f"  Cluster {cluster}: {count:,} å®¢æˆ· ({percentage:.1f}%)")

# ä¿å­˜æ¨¡å‹å‚æ•°ä¾›åç»­ä½¿ç”¨
cluster_centers = scaler.inverse_transform(kmeans_final.cluster_centers_)
cluster_centers_df = pd.DataFrame(cluster_centers, columns=selected_features)
print(f"\n=== èšç±»ä¸­å¿ƒç‰¹å¾å€¼ ===")
print(cluster_centers_df.round(2))
                

# æ·±åº¦åˆ†æå„èšç±»ç‰¹å¾
def analyze_clusters(rfm_enhanced):
    """
    è¯¦ç»†åˆ†æå„ä¸ªèšç±»çš„ç‰¹å¾
    """
    cluster_summary = rfm_enhanced.groupby('Cluster').agg({
        'Recency': ['mean', 'median'],
        'Frequency': ['mean', 'median'], 
        'Monetary': ['mean', 'median'],
        'AvgOrderValue': ['mean', 'median'],
        'CLV_Score': ['mean', 'median'],
        'ProductVariety': ['mean', 'median'],
        'ActivityRate': ['mean', 'median'],
        'CustomerLifetime': ['mean', 'median']
    }).round(2)
    
    # æ‰å¹³åŒ–åˆ—å
    cluster_summary.columns = [f"{col[0]}_{col[1]}" for col in cluster_summary.columns]
    
    return cluster_summary

# ç”Ÿæˆèšç±»åˆ†ææŠ¥å‘Š
cluster_analysis = analyze_clusters(rfm_enhanced)
print("=== å„èšç±»è¯¦ç»†ç‰¹å¾åˆ†æ ===")
print(cluster_analysis)

# ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆå®¢æˆ·ä»·å€¼è¯„ä¼°
def evaluate_cluster_value(rfm_enhanced):
    """
    è¯„ä¼°å„èšç±»çš„å•†ä¸šä»·å€¼
    """
    cluster_value = rfm_enhanced.groupby('Cluster').agg({
        'Monetary': ['sum', 'mean'],
        'Frequency': 'mean',
        'Recency': 'mean',
        'CLV_Score': 'mean',
        'ActivityRate': 'mean'
    }).round(2)
    
    # è®¡ç®—å„èšç±»å æ€»æ”¶å…¥æ¯”ä¾‹
    total_revenue = rfm_enhanced['Monetary'].sum()
    cluster_revenue_pct = (rfm_enhanced.groupby('Cluster')['Monetary'].sum() / total_revenue * 100).round(1)
    
    # è®¡ç®—å®¢æˆ·æ•°é‡å æ¯”
    total_customers = len(rfm_enhanced)
    cluster_customer_pct = (rfm_enhanced['Cluster'].value_counts().sort_index() / total_customers * 100).round(1)
    
    return cluster_value, cluster_revenue_pct, cluster_customer_pct

cluster_value, revenue_pct, customer_pct = evaluate_cluster_value(rfm_enhanced)

print(f"\n=== èšç±»å•†ä¸šä»·å€¼è¯„ä¼° ===")
for i in range(5):
    print(f"\nCluster {i}:")
    print(f"  å®¢æˆ·å æ¯”: {customer_pct[i]}%")
    print(f"  æ”¶å…¥å æ¯”: {revenue_pct[i]}%")
    print(f"  å¹³å‡å®¢æˆ·ä»·å€¼: Â£{cluster_value.loc[i, ('Monetary', 'mean')]:,.2f}")
    print(f"  å¹³å‡è´­ä¹°é¢‘æ¬¡: {cluster_value.loc[i, ('Frequency', 'mean')]:.1f}")
    print(f"  å¹³å‡è·ä»Šå¤©æ•°: {cluster_value.loc[i, ('Recency', 'mean')]:.0f}å¤©")
                
# ç”Ÿæˆå®¢æˆ·åˆ†ç¾¤æ ‡ç­¾å­—å…¸
cluster_labels_dict = {
    0: "å† å†›å®¢æˆ· (Champions)",
    1: "å¿ è¯šå®¢æˆ· (Loyal Customers)", 
    2: "æ½œåŠ›å®¢æˆ· (Potential Loyalists)",
    3: "æ–°å®¢æˆ· (New Customers)",
    4: "æµå¤±é£é™©å®¢æˆ· (At Risk)"
}

# åº”ç”¨æ ‡ç­¾
rfm_enhanced['CustomerSegment'] = rfm_enhanced['Cluster'].map(cluster_labels_dict)

# ä¿å­˜åˆ†ç¾¤ç»“æœ
print("=== åˆ†ç¾¤ç»“æœæ ·ä¾‹ ===")
sample_results = rfm_enhanced[['Recency', 'Frequency', 'Monetary', 'Cluster', 'CustomerSegment']].head(10)
print(sample_results)

# å¯¼å‡ºç»“æœä¾›åç»­è¥é”€ä½¿ç”¨
rfm_enhanced.to_csv('customer_segmentation_results.csv', index=True)
print(f"\nåˆ†ç¾¤ç»“æœå·²ä¿å­˜åˆ° 'customer_segmentation_results.csv'")
print(f"åŒ…å« {len(rfm_enhanced)} ä¸ªå®¢æˆ·çš„è¯¦ç»†åˆ†ç¾¤ä¿¡æ¯")
# åŸºäºçœŸå®æ•°æ®çš„è¥é”€é¢„ç®—åˆ†é…è®¡ç®—
def calculate_marketing_budget(rfm_enhanced, total_budget=100000):
    """
    åŸºäºå®¢æˆ·ä»·å€¼å’ŒROIæ½œåŠ›åˆ†é…è¥é”€é¢„ç®—
    """
    # è®¡ç®—å„ç¾¤ä½“çš„å•†ä¸šä»·å€¼æŒ‡æ ‡
    cluster_metrics = rfm_enhanced.groupby('Cluster').agg({
        'Monetary': ['sum', 'mean', 'count'],
        'CLV_Score': 'mean',
        'ActivityRate': 'mean'
    }).round(2)
    
    # è®¡ç®—é¢„ç®—åˆ†é…æƒé‡
    revenue_weight = 0.4  # å½“å‰æ”¶å…¥è´¡çŒ®æƒé‡
    potential_weight = 0.3  # å¢é•¿æ½œåŠ›æƒé‡  
    count_weight = 0.3  # å®¢æˆ·æ•°é‡æƒé‡
    
    budget_allocation = {}
    
    for cluster in range(5):
        revenue_contribution = cluster_metrics.loc[cluster, ('Monetary', 'sum')] / rfm_enhanced['Monetary'].sum()
        customer_count_ratio = cluster_metrics.loc[cluster, ('Monetary', 'count')] / len(rfm_enhanced)
        clv_ratio = cluster_metrics.loc[cluster, ('CLV_Score', 'mean')] / rfm_enhanced['CLV_Score'].mean()
        
        # ç»¼åˆæƒé‡è®¡ç®—
        weight = (revenue_contribution * revenue_weight + 
                 customer_count_ratio * count_weight + 
                 clv_ratio * potential_weight)
        
        budget_allocation[cluster] = weight
    
    # å½’ä¸€åŒ–é¢„ç®—åˆ†é…
    total_weight = sum(budget_allocation.values())
    budget_final = {k: (v/total_weight * total_budget) for k, v in budget_allocation.items()}
    
    return budget_final

# è®¡ç®—è¥é”€é¢„ç®—åˆ†é…
marketing_budget = calculate_marketing_budget(rfm_enhanced, total_budget=500000)

print("=== è¥é”€é¢„ç®—åˆ†é…å»ºè®®ï¼ˆå¹´åº¦50ä¸‡é¢„ç®—ï¼‰===")
cluster_names = ["å† å†›å®¢æˆ·", "å¿ è¯šå®¢æˆ·", "æ½œåŠ›å®¢æˆ·", "æ–°å®¢æˆ·", "æµå¤±é£é™©"]
for i, (cluster, budget) in enumerate(marketing_budget.items()):
    percentage = budget / 500000 * 100
    print(f"{cluster_names[i]}: Â£{budget:,.0f} ({percentage:.1f}%)")
# è¥é”€æ•ˆæœé¢„æµ‹æ¨¡å‹
def predict_marketing_roi(rfm_enhanced, marketing_budget):
    """
    åŸºäºå†å²æ•°æ®é¢„æµ‹è¥é”€æ´»åŠ¨çš„ROI
    """
    # å„ç¾¤ä½“çš„è½¬åŒ–ç‡å’Œæå‡æ½œåŠ›ï¼ˆåŸºäºè¡Œä¸šç»éªŒå’Œæ•°æ®åˆ†æï¼‰
    conversion_rates = {
        0: 0.85,  # å† å†›å®¢æˆ·ï¼šé«˜ç•™å­˜ï¼Œé‡å¤è´­ä¹°
        1: 0.70,  # å¿ è¯šå®¢æˆ·ï¼šç¨³å®šè½¬åŒ–
        2: 0.45,  # æ½œåŠ›å®¢æˆ·ï¼šä¸­ç­‰è½¬åŒ–ç‡
        3: 0.35,  # æ–°å®¢æˆ·ï¼šè¾ƒä½è½¬åŒ–ç‡ä½†é‡å¤§
        4: 0.25   # æµå¤±é£é™©ï¼šä½è½¬åŒ–ç‡
    }
    
    # å®¢å•ä»·æå‡æ½œåŠ›
    aov_increase = {
        0: 1.15,  # å† å†›å®¢æˆ·ï¼š15%æå‡
        1: 1.25,  # å¿ è¯šå®¢æˆ·ï¼š25%æå‡  
        2: 1.40,  # æ½œåŠ›å®¢æˆ·ï¼š40%æå‡
        3: 1.60,  # æ–°å®¢æˆ·ï¼š60%æå‡
        4: 1.20   # æµå¤±é£é™©ï¼š20%æå‡
    }
    
    # è´­ä¹°é¢‘æ¬¡æå‡æ½œåŠ›
    frequency_increase = {
        0: 1.10,  # å† å†›å®¢æˆ·ï¼š10%æå‡
        1: 1.30,  # å¿ è¯šå®¢æˆ·ï¼š30%æå‡
        2: 1.80,  # æ½œåŠ›å®¢æˆ·ï¼š80%æå‡  
        3: 2.50,  # æ–°å®¢æˆ·ï¼š150%æå‡
        4: 1.50   # æµå¤±é£é™©ï¼š50%æå‡
    }
    
    roi_predictions = {}
    total_predicted_revenue = 0
    total_marketing_cost = sum(marketing_budget.values())
    
    for cluster in range(5):
        cluster_data = rfm_enhanced[rfm_enhanced['Cluster'] == cluster]
        customer_count = len(cluster_data)
        avg_monetary = cluster_data['Monetary'].mean()
        avg_frequency = cluster_data['Frequency'].mean()
        
        # è®¡ç®—é¢„æœŸæ”¶å…¥å¢é•¿
        activated_customers = customer_count * conversion_rates[cluster]
        new_aov = avg_monetary / avg_frequency * aov_increase[cluster]
        new_frequency = avg_frequency * frequency_increase[cluster]
        
        # é¢„æµ‹å¢é‡æ”¶å…¥
        baseline_revenue = customer_count * avg_monetary
        predicted_revenue = activated_customers * new_aov * new_frequency
        incremental_revenue = predicted_revenue - baseline_revenue
        
        # ROIè®¡ç®—
        cluster_roi = incremental_revenue / marketing_budget[cluster] if marketing_budget[cluster] > 0 else 0
        
        roi_predictions[cluster] = {
            'customer_count': customer_count,
            'activated_customers': int(activated_customers),
            'baseline_revenue': baseline_revenue,
            'predicted_revenue': predicted_revenue,
            'incremental_revenue': incremental_revenue,
            'marketing_spend': marketing_budget[cluster],
            'roi': cluster_roi
        }
        
        total_predicted_revenue += incremental_revenue
    
    # æ•´ä½“ROI
    overall_roi = total_predicted_revenue / total_marketing_cost
    
    return roi_predictions, overall_roi

# æ‰§è¡ŒROIé¢„æµ‹
roi_results, overall_roi = predict_marketing_roi(rfm_enhanced, marketing_budget)

print("=== è¥é”€ROIé¢„æµ‹åˆ†æ ===")
cluster_names = ["å† å†›å®¢æˆ·", "å¿ è¯šå®¢æˆ·", "æ½œåŠ›å®¢æˆ·", "æ–°å®¢æˆ·", "æµå¤±é£é™©"]

for i, (cluster, results) in enumerate(roi_results.items()):
    print(f"\n{cluster_names[i]} (Cluster {cluster}):")
    print(f"  ç›®æ ‡å®¢æˆ·æ•°: {results['customer_count']:,}")
    print(f"  é¢„æœŸæ¿€æ´»æ•°: {results['activated_customers']:,}")
    print(f"  è¥é”€æŠ•å…¥: Â£{results['marketing_spend']:,.0f}")
    print(f"  é¢„æœŸå¢é‡æ”¶å…¥: Â£{results['incremental_revenue']:,.0f}")
    print(f"  ROI: {results['roi']:.2f}x")

print(f"\n=== æ•´ä½“é¢„æµ‹æ•ˆæœ ===")
print(f"æ€»è¥é”€æŠ•å…¥: Â£{sum(marketing_budget.values()):,.0f}")
print(f"é¢„æœŸå¢é‡æ”¶å…¥: Â£{total_predicted_revenue:,.0f}")
print(f"æ•´ä½“ROI: {overall_roi:.2f}x")
print(f"æŠ•èµ„å›æŠ¥ç‡: {(overall_roi-1)*100:.1f}%")

# æ„å»ºè¥é”€æ•ˆæœç›‘æ§ä½“ç³»
def create_monitoring_system():
    """
    å»ºç«‹åˆ†ç¾¤ä½“è¥é”€æ•ˆæœç›‘æ§æŒ‡æ ‡ä½“ç³»
    """
    monitoring_metrics = {
        'daily_metrics': {
            'conversion_rate': 'å„ç¾¤ä½“æ—¥è½¬åŒ–ç‡',
            'campaign_ctr': 'è¥é”€æ´»åŠ¨ç‚¹å‡»ç‡', 
            'revenue_per_customer': 'äººå‡è´¡çŒ®æ”¶å…¥',
            'cost_per_acquisition': 'è·å®¢æˆæœ¬'
        },
        
        'weekly_metrics': {
            'customer_upgrade_rate': 'å®¢æˆ·ç¾¤ä½“å‡çº§ç‡',
            'retention_rate': 'å®¢æˆ·ç•™å­˜ç‡',
            'average_order_value': 'å¹³å‡è®¢å•ä»·å€¼',
            'campaign_roi': 'è¥é”€æ´»åŠ¨ROI'
        },
        
        'monthly_metrics': {
            'ltv_growth': 'å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼å¢é•¿',
            'churn_rate': 'å®¢æˆ·æµå¤±ç‡',
            'cross_sell_success': 'äº¤å‰é”€å”®æˆåŠŸç‡',
            'customer_satisfaction': 'å®¢æˆ·æ»¡æ„åº¦'
        },
        
        'alert_thresholds': {
            'conversion_rate_drop': 0.2,  # è½¬åŒ–ç‡ä¸‹é™20%é¢„è­¦
            'cac_increase': 0.3,          # è·å®¢æˆæœ¬ä¸Šå‡30%é¢„è­¦
            'churn_rate_spike': 0.15,     # æµå¤±ç‡çªå¢15%é¢„è­¦
            'roi_below_target': 2.0       # ROIä½äº2.0é¢„è­¦
        }
    }
    
    return monitoring_metrics

# å®æ—¶ç›‘æ§ä»ªè¡¨æ¿è®¾è®¡
def design_dashboard():
    """
    è®¾è®¡è¥é”€æ•ˆæœç›‘æ§ä»ªè¡¨æ¿
    """
    dashboard_components = {
        'overview_section': [
            'æ•´ä½“ROIè¶‹åŠ¿å›¾',
            'å„ç¾¤ä½“æ”¶å…¥è´¡çŒ®é¥¼å›¾', 
            'å®¢æˆ·ç¾¤ä½“æµè½¬æ¡‘åŸºå›¾',
            'å…³é”®æŒ‡æ ‡KPIå¡ç‰‡'
        ],
        
        'cluster_detail_section': [
            'å„ç¾¤ä½“è½¬åŒ–æ¼æ–—',
            'è¥é”€æ´»åŠ¨æ•ˆæœå¯¹æ¯”',
            'å®¢æˆ·ä»·å€¼åˆ†å¸ƒçƒ­åŠ›å›¾',
            'ç•™å­˜ç‡cohortåˆ†æ'
        ],
        
        'alert_section': [
            'å¼‚å¸¸æŒ‡æ ‡é¢„è­¦åˆ—è¡¨',
            'ä¼˜åŒ–å»ºè®®æ¨é€',
            'A/Bæµ‹è¯•ç»“æœå±•ç¤º',
            'ç«å“å¯¹æ¯”åˆ†æ'
        ]
    }
    
    return dashboard_components

monitoring_system = create_monitoring_system()
dashboard_design = design_dashboard()

print("=== ç›‘æ§ä½“ç³»è®¾è®¡ ===")
print("ğŸ“Š æ—¥å¸¸ç›‘æ§æŒ‡æ ‡:")
for metric, desc in monitoring_system['daily_metrics'].items():
    print(f"  â€¢ {desc} ({metric})")

print(f"\nâš ï¸ é¢„è­¦é˜ˆå€¼è®¾ç½®:")
for threshold, value in monitoring_system['alert_thresholds'].items():
    print(f"  â€¢ {threshold}: {value}")
