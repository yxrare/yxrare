# Risk Modeling and Validation Strategy Analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix

# Loading transaction and user data
transactions = pd.read_csv('transactions.csv')
user_profiles = pd.read_csv('user_profiles.csv')

# Data preprocessing and feature engineering
def preprocess_data(transactions, user_profiles):
    # Merging user profiles and transaction data
    data = pd.merge(transactions, user_profiles, on='user_id')
    
    # Creating Behavioral Characteristics
    data['hour_of_day'] = pd.to_datetime(data['transaction_time']).dt.hour
    data['day_of_week'] = pd.to_datetime(data['transaction_time']).dt.dayofweek
    
    # Calculate historical user transaction statistics
    user_stats = transactions.groupby('user_id').agg({
        'transaction_id': 'count',
        'amount': ['mean', 'std', 'max'],
        'merchant_id': lambda x: x.nunique(),
        'is_fraud': 'sum'
    }).reset_index()
    
    user_stats.columns = ['user_id', 'tx_count', 'avg_amount', 'std_amount', 
                          'max_amount', 'merchant_count', 'fraud_count']
    
    # Merge back to master data
    data = pd.merge(data, user_stats, on='user_id')
    
    # User risk score (example)
    data['user_risk_score'] = (
        data['fraud_count'] / (data['tx_count'] + 1) * 50 + 
        data['amount'] / data['avg_amount'] * 30 +
        (data['new_device'] == 1) * 20
    )
    
    # Scene Characteristics
    data['is_familiar_merchant'] = data.apply(
        lambda row: 1 if row['merchant_id'] in 
        transactions[transactions['user_id'] == row['user_id']]['merchant_id'].value_counts().head(5).index 
        else 0, axis=1)
    
    # Amount anomaly
    data['amount_zscore'] = (data['amount'] - data['avg_amount']) / (data['std_amount'] + 1)
    
    return data

# risk stratification model
def build_risk_segmentation_model(data):
    # feature selection
    features = ['tx_count', 'avg_amount', 'std_amount', 'merchant_count', 
                'user_risk_score', 'amount_zscore', 'is_familiar_merchant',
                'new_device', 'hour_of_day', 'day_of_week']
    
    X = data[features]
    y = data['is_fraud']
    
    # training model
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Predicting the probability of risk
    data['risk_probability'] = model.predict_proba(X)[:, 1]
    
    # risk stratification
    data['risk_segment'] = pd.qcut(data['risk_probability'], 
                                [0, 0.5, 0.85, 1], 
                                labels=['Low', 'Medium', 'High'])
    
    return data, model

# Validation strategy optimization
def optimize_verification_strategy(data, model):
    # Define the efficiency matrix for different verification strategies
    verification_efficiency = {
        'no_verification': {'fraud_detection': 0.0, 'user_friction': 0.0},
        'device_check': {'fraud_detection': 0.73, 'user_friction': 0.2},
        'behavioral': {'fraud_detection': 0.81, 'user_friction': 0.3},
        'single_factor': {'fraud_detection': 0.85, 'user_friction': 0.5},
        'two_factor': {'fraud_detection': 0.92, 'user_friction': 0.7},
        'multi_factor': {'fraud_detection': 0.98, 'user_friction': 0.9}
    }
    
    # Authentication policy mapping based on user risk stratification and scenarios
    def get_verification_strategy(row):
        # low risk user
        if row['risk_segment'] == 'Low':
            if row['is_familiar_merchant'] == 1 and row['amount_zscore'] < 1:
                return 'no_verification'  # Familiarity with merchants + normal amount
            elif row['is_familiar_merchant'] == 1:
                return 'device_check'  # Familiarity with merchants + unusual amounts
            else:
                return 'single_factor'  # Unfamiliar Merchants
        
        # mid risk user
        elif row['risk_segment'] == 'Medium':
            if row['is_familiar_merchant'] == 1 and row['amount_zscore'] < 1:
                return 'device_check'  # Familiarity with merchants + normal amount
            elif row['is_familiar_merchant'] == 1:
                return 'single_factor'  # Familiarity with merchants + unusual amounts
            else:
                return 'two_factor'  # Unfamiliar Merchants
                
        # high risk user
        else:  # 'High'
            if row['is_familiar_merchant'] == 1 and row['amount_zscore'] < 1:
                return 'single_factor'  # Familiarity with merchants + normal amount
            elif row['is_familiar_merchant'] == 1:
                return 'two_factor'  # Familiarity with merchants + unusual amounts
            else:
                return 'multi_factor'  # Unfamiliar Merchants
    
    # Apply strategies and calculate metrics
    data['verification_strategy'] = data.apply(get_verification_strategy, axis=1)
    
    # Calculate the volume share for each strategy
    strategy_distribution = data['verification_strategy'].value_counts(normalize=True)
    
    # Calculate average friction and detection rate
    avg_friction = sum(data['verification_strategy'].map(
        lambda x: verification_efficiency[x]['user_friction']) * 1) / len(data)
    
    avg_detection = sum(data['verification_strategy'].map(
        lambda x: verification_efficiency[x]['fraud_detection']) * 
                      data['risk_probability']) / sum(data['risk_probability'])
    
    print(f"Strategy Distribution: {strategy_distribution}")
    print(f"Average user friction: {avg_friction:.2f}")
    print(f"Estimated fraud detection rate: {avg_detection:.2f}")
    
    return data, strategy_distribution, avg_friction, avg_detection

# Visualization of analysis results
def plot_verification_results(data, strategy_distribution):
    # Strategy Distribution Pie Chart
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 2, 1)
    strategy_distribution.plot(kind='pie', autopct='%1.1f%%', 
                             colors=sns.color_palette("viridis", 6),
                             startangle=90)
    plt.title('Validation Strategy Distribution')
    plt.ylabel('')
    
    # Risk Stratification and Validation Strategy Heat Map
    plt.subplot(2, 2, 2)
    risk_vs_strategy = pd.crosstab(data['risk_segment'], data['verification_strategy'], 
                                  normalize='index')
    sns.heatmap(risk_vs_strategy, annot=True, cmap='YlGnBu', fmt='.1%')
    plt.title('Risk Stratification and Validation Strategy Distribution')
    
    # User Friction vs. Detection Rate
    plt.subplot(2, 2, 3)
    verification_types = list(verification_efficiency.keys())
    friction = [verification_efficiency[v]['user_friction'] for v in verification_types]
    detection = [verification_efficiency[v]['fraud_detection'] for v in verification_types]
    
    x = range(len(verification_types))
    width = 0.35
    
    plt.bar([i - width/2 for i in x], friction, width, label='User Friction', color='#3498db')
    plt.bar([i + width/2 for i in x], detection, width, label='Fraud detection rate', color='#e74c3c')
    
    plt.xticks(x, verification_types, rotation=45)
    plt.legend()
    plt.title('Comparison of friction and detection rate of validation methods')
    
    # Comparison of the effectiveness of validation strategies
    plt.subplot(2, 2, 4)
    # Compare existing strategies  with optimized strategies
    current_friction = 0.7  # Assuming the current friction level using two_factor
    current_detection = 0.92  # Set the current detection rate using two_factor
    
    comparison = pd.DataFrame({
        'Strategy': ['Current', 'Optimized'],
        'User Friction': [current_friction, avg_friction],
        'Fraud Detection': [current_detection, avg_detection]
    })
    
    sns.barplot(x='Strategy', y='value', hue='variable', 
                data=pd.melt(comparison, id_vars=['Strategy'], 
                            value_vars=['User Friction', 'Fraud Detection']))
    plt.title('Comparison of the effect before and after optimization')
    
    plt.tight_layout()
    plt.show()
